{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e82de63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "from sagemaker.session import s3_input , Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4ef6adcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eu-north-1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "my_region=boto3.session.Session().region_name\n",
    "print(my_region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "97f5ee4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 bucket created successfully\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "my_region = 'eu-north-1'\n",
    "bucket_name = 'bankapplicationnew12'\n",
    "if my_region == 'eu-north-1':\n",
    "    s3.create_bucket(\n",
    "        Bucket=bucket_name,\n",
    "        CreateBucketConfiguration={'LocationConstraint': my_region}\n",
    "    )\n",
    "else:\n",
    "    s3.create_bucket(Bucket=bucket_name)\n",
    "\n",
    "print(\"S3 bucket created successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "52f269ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data download success\n"
     ]
    }
   ],
   "source": [
    "# Download data\n",
    "try:\n",
    "    urllib.request.urlretrieve(\n",
    "        \"https://d1.awsstatic.com/tmt/build-train-deploy-machine-learning-model-sagemaker/bank_clean.27f01fbbdf43271788427f3682996ae29ceca05d.csv\",\n",
    "        \"bank_clean.csv\"\n",
    "    )\n",
    "    print(\"Data download success\")\n",
    "except Exception as e:\n",
    "    print(\"Data load error\", e)\n",
    "\n",
    "# Load data\n",
    "model_data = pd.read_csv('./bank_clean.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f4c51c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/numpy/core/fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28831, 61) (12357, 61)\n",
      "Training data columns: Index(['age', 'campaign', 'pdays', 'previous', 'no_previous_contact',\n",
      "       'not_working', 'job_admin.', 'job_blue-collar', 'job_entrepreneur',\n",
      "       'job_housemaid', 'job_management', 'job_retired', 'job_self-employed',\n",
      "       'job_services', 'job_student', 'job_technician', 'job_unemployed',\n",
      "       'job_unknown', 'marital_divorced', 'marital_married', 'marital_single',\n",
      "       'marital_unknown', 'education_basic.4y', 'education_basic.6y',\n",
      "       'education_basic.9y', 'education_high.school', 'education_illiterate',\n",
      "       'education_professional.course', 'education_university.degree',\n",
      "       'education_unknown', 'default_no', 'default_unknown', 'default_yes',\n",
      "       'housing_no', 'housing_unknown', 'housing_yes', 'loan_no',\n",
      "       'loan_unknown', 'loan_yes', 'contact_cellular', 'contact_telephone',\n",
      "       'month_apr', 'month_aug', 'month_dec', 'month_jul', 'month_jun',\n",
      "       'month_mar', 'month_may', 'month_nov', 'month_oct', 'month_sep',\n",
      "       'day_of_week_fri', 'day_of_week_mon', 'day_of_week_thu',\n",
      "       'day_of_week_tue', 'day_of_week_wed', 'poutcome_failure',\n",
      "       'poutcome_nonexistent', 'poutcome_success', 'y_no', 'y_yes'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Split data into training and testing\n",
    "import numpy as np\n",
    "train_data, test_data = np.split(model_data.sample(frac=1, random_state=1729), [int(0.7 * len(model_data))])\n",
    "print(train_data.shape, test_data.shape)\n",
    "\n",
    "# Save training and testing data to CSV\n",
    "train_data.to_csv('train.csv', index=False, header=True)\n",
    "test_data.to_csv('test.csv', index=False, header=True)\n",
    "\n",
    "# Inspect the columns in the training data\n",
    "train_df = pd.read_csv('train.csv')\n",
    "print(\"Training data columns:\", train_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "484fb0dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data uploaded to S3: s3://bankapplicationnew12/xgboost-as-a-built-in-algo/train/train_libsvm.txt\n",
      "Testing data uploaded to S3: s3://bankapplicationnew12/xgboost-as-a-built-in-algo/validation/test_libsvm.txt\n"
     ]
    }
   ],
   "source": [
    "# Replace 'y_yes' with the correct label column name if different\n",
    "label_column = 'y_yes'\n",
    "\n",
    "# Function to convert CSV to LIBSVM format\n",
    "def csv_to_libsvm(input_csv_path, output_libsvm_path, label_column):\n",
    "    data = pd.read_csv(input_csv_path)\n",
    "    y = data[label_column]\n",
    "    X = data.drop(columns=[label_column])\n",
    "    dump_svmlight_file(X, y, output_libsvm_path, zero_based=True)\n",
    "\n",
    "# Convert training and testing data to LIBSVM format\n",
    "csv_to_libsvm('train.csv', 'train_libsvm.txt', label_column)\n",
    "csv_to_libsvm('test.csv', 'test_libsvm.txt', label_column)\n",
    "\n",
    "# Upload converted data to S3\n",
    "train_libsvm_s3_path = os.path.join(prefix, 'train/train_libsvm.txt')\n",
    "s3.Bucket(bucket_name).Object(train_libsvm_s3_path).upload_file('train_libsvm.txt')\n",
    "\n",
    "test_libsvm_s3_path = os.path.join(prefix, 'validation/test_libsvm.txt')\n",
    "s3.Bucket(bucket_name).Object(test_libsvm_s3_path).upload_file('test_libsvm.txt')\n",
    "\n",
    "print(f\"Training data uploaded to S3: s3://{bucket_name}/{train_libsvm_s3_path}\")\n",
    "print(f\"Testing data uploaded to S3: s3://{bucket_name}/{test_libsvm_s3_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8eca07cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Defaulting to only available Python version: py3\n",
      "INFO:sagemaker.image_uris:Defaulting to only supported image scope: cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 input for SageMaker created successfully\n"
     ]
    }
   ],
   "source": [
    "# Prepare S3 input for SageMaker\n",
    "s3_input_train = TrainingInput(s3_data=f's3://{bucket_name}/{train_libsvm_s3_path}', content_type='libsvm')\n",
    "s3_input_test = TrainingInput(s3_data=f's3://{bucket_name}/{test_libsvm_s3_path}', content_type='libsvm')\n",
    "print(\"S3 input for SageMaker created successfully\")\n",
    "\n",
    "# Define container and hyperparameters\n",
    "container = sagemaker.image_uris.retrieve(\n",
    "    framework='xgboost',\n",
    "    region=boto3.Session().region_name,\n",
    "    version='1.0-1'\n",
    ")\n",
    "hyperparameters = {\n",
    "    \"max_depth\": \"5\",\n",
    "    \"eta\": \"0.2\",\n",
    "    \"gamma\": \"4\",\n",
    "    \"min_child_weight\": \"6\",\n",
    "    \"subsample\": \"0.7\",\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"num_round\": \"100\"  # Add this line\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c1eb0a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: sagemaker-xgboost-2024-07-05-06-15-04-635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-05 06:15:04 Starting - Starting the training job...\n",
      "2024-07-05 06:15:21 Starting - Preparing the instances for training...\n",
      "2024-07-05 06:15:57 Downloading - Downloading the training image...\n",
      "2024-07-05 06:16:33 Training - Training image download completed. Training in progress....\n",
      "2024-07-05 06:16:53 Uploading - Uploading generated training model\u001b[34m[2024-07-05 06:16:48.184 ip-10-0-194-137.eu-north-1.compute.internal:7 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Failed to parse hyperparameter objective value binary:logistic to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34mINFO:sagemaker_xgboost_container.training:Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[34mINFO:root:Single node training.\u001b[0m\n",
      "\u001b[34m[06:16:48] 28831x60 matrix with 434598 entries loaded from /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34m[06:16:48] 12357x60 matrix with 186286 entries loaded from /opt/ml/input/data/validation\u001b[0m\n",
      "\u001b[34m[2024-07-05 06:16:48.282 ip-10-0-194-137.eu-north-1.compute.internal:7 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2024-07-05 06:16:48.283 ip-10-0-194-137.eu-north-1.compute.internal:7 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2024-07-05 06:16:48.283 ip-10-0-194-137.eu-north-1.compute.internal:7 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2024-07-05 06:16:48.283 ip-10-0-194-137.eu-north-1.compute.internal:7 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2024-07-05 06:16:48.283 ip-10-0-194-137.eu-north-1.compute.internal:7 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34mINFO:root:Debug hook created from config\u001b[0m\n",
      "\u001b[34mINFO:root:Train matrix has 28831 rows\u001b[0m\n",
      "\u001b[34mINFO:root:Validation matrix has 12357 rows\u001b[0m\n",
      "\u001b[34m[06:16:48] WARNING: /workspace/src/learner.cc:328: \u001b[0m\n",
      "\u001b[34mParameters: { num_round } might not be used.\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\u001b[0m\n",
      "\u001b[34m[0]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\u001b[34m[2024-07-05 06:16:48.296 ip-10-0-194-137.eu-north-1.compute.internal:7 INFO hook.py:423] Monitoring the collections: metrics\u001b[0m\n",
      "\u001b[34m[2024-07-05 06:16:48.298 ip-10-0-194-137.eu-north-1.compute.internal:7 INFO hook.py:486] Hook is writing from the hook with pid: 7\u001b[0m\n",
      "\u001b[34m[1]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\u001b[34m[2]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\u001b[34m[3]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\u001b[34m[4]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\u001b[34m[5]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\u001b[34m[6]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\u001b[34m[7]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\u001b[34m[8]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\u001b[34m[9]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\u001b[34m[10]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\u001b[34m[11]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\u001b[34m[12]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\u001b[34m[13]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\u001b[34m[14]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\u001b[34m[15]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\u001b[34m[16]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\u001b[34m[17]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\u001b[34m[18]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\u001b[34m[19]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\u001b[34m[20]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\u001b[34m[21]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\u001b[34m[22]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\u001b[34m[23]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\u001b[34m[24]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\u001b[34m[25]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\u001b[34m[26]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\u001b[34m[27]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\u001b[34m[28]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\u001b[34m[29]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\u001b[34m[30]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\u001b[34m[31]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\u001b[34m[32]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\u001b[34m[33]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\u001b[34m[34]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\u001b[34m[35]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\u001b[34m[36]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\u001b[34m[37]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\u001b[34m[38]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\u001b[34m[39]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\u001b[34m[40]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\u001b[34m[41]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\u001b[34m[42]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\u001b[34m[43]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\u001b[34m[44]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\u001b[34m[45]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\u001b[34m[46]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\u001b[34m[47]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\u001b[34m[48]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\u001b[34m[49]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\u001b[34m[50]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\u001b[34m[51]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\u001b[34m[52]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\u001b[34m[53]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\u001b[34m[54]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\u001b[34m[55]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\u001b[34m[56]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\u001b[34m[57]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\u001b[34m[58]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\u001b[34m[59]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\u001b[34m[60]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\u001b[34m[61]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\u001b[34m[62]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\u001b[34m[63]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\u001b[34m[64]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\u001b[34m[65]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\u001b[34m[66]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\u001b[34m[67]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\u001b[34m[68]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\u001b[34m[69]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\u001b[34m[70]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\u001b[34m[71]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\u001b[34m[72]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\u001b[34m[73]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\u001b[34m[74]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\u001b[34m[75]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\u001b[34m[76]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\u001b[34m[77]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\u001b[34m[78]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\u001b[34m[79]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\u001b[34m[80]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\u001b[34m[81]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\u001b[34m[82]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\u001b[34m[83]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\u001b[34m[84]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\u001b[34m[85]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\u001b[34m[86]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\u001b[34m[87]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\u001b[34m[88]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\u001b[34m[89]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\u001b[34m[90]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\u001b[34m[91]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\u001b[34m[92]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\u001b[34m[93]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\u001b[34m[94]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\u001b[34m[95]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\u001b[34m[96]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\u001b[34m[97]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\u001b[34m[98]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\u001b[34m[99]#011train-error:0.00000#011validation-error:0.00000\u001b[0m\n",
      "\n",
      "2024-07-05 06:17:06 Completed - Training job completed\n",
      "Training seconds: 84\n",
      "Billable seconds: 25\n",
      "Managed Spot Training savings: 70.2%\n"
     ]
    }
   ],
   "source": [
    "# Initialize and fit estimator\n",
    "role = sagemaker.get_execution_role()\n",
    "estimator = sagemaker.estimator.Estimator(\n",
    "    image_uri=container,\n",
    "    hyperparameters=hyperparameters,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.2xlarge\",\n",
    "    volume_size=5,\n",
    "    output_path=output_path,\n",
    "    use_spot_instances=True,\n",
    "    max_run=300,\n",
    "    max_wait=600\n",
    ")\n",
    "\n",
    "estimator.fit({'train': s3_input_train, 'validation': s3_input_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72a0ec3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
